{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea01e3a-31da-43b6-b54e-997ec7906edf",
   "metadata": {},
   "source": [
    "# Setting Up The Environment & Testing It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7040a57-3005-4c77-b26b-6c8fb4e1c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221dea7a-0082-4d9d-9ffd-1c4fc6732876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if env was registered: True\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.envs import registry\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0', # make sure this is a custom name!\n",
    "    entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100, # how many steps the agent will take before just giving up\n",
    "    reward_threshold=.8196, # this is more applicable for continuous rewards\n",
    ")\n",
    "\n",
    "print(f\"Check if env was registered: {'FrozenLakeNotSlippery-v0' in registry.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f46191-a915-4d5a-9ddc-d856b97a4ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# You can view this game multiple ways:\n",
    "# human (graphical game window)\n",
    "# rgb_array (pixel data)\n",
    "# ansi (text based)\n",
    "render_mode = \"ansi\"\n",
    "env = gym.make('FrozenLakeNotSlippery-v0', render_mode=render_mode)\n",
    "env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    # render environment accordingly\n",
    "    if render_mode == \"human\":\n",
    "        env.render()\n",
    "    else:\n",
    "        img = env.render()\n",
    "        if render_mode == \"ansi\":\n",
    "            print(img)\n",
    "        else:\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    # select a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # go forward with this action\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66da5d-fa57-4ab7-9705-40e38f2ad2f9",
   "metadata": {},
   "source": [
    "# Hyperparameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da1446e-2751-49f8-85e3-0c077291ede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size = 16\n",
      "action size = 4\n",
      "q-table = [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# rows -> states, cols -> actions(?)\n",
    "state_size = env.observation_space.n \n",
    "print(f'state size = {state_size}')\n",
    "action_size = env.action_space.n\n",
    "print(f'action size = {action_size}')\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "print(f'q-table = {q_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9f4b48-5bb4-472b-96ea-4955c16e4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also referred to as EPOCHS, \n",
    "# running for a few thousands is how many times \n",
    "# it takes the agent of playing the game to start showing results\n",
    "EPISODES = 20000 \n",
    "\n",
    "# The LEARNING RATE, too low converges too fast, too high converges too quick\n",
    "ALPHA = 0.8\n",
    "\n",
    "# The DISCOUNT RATE, applied to 'future' rewards, so that recent rewards are worth more\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1282122c-1242-4a65-bfae-e21844f90742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the epsilon-greedy algorithm\n",
    "# Epsilon will start at the max value and will decrease to no less than the min value\n",
    "# based on a function that we choose (in this case exponential decay)\n",
    "# and we also state the rate of decay\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988ea04-fbab-4812-b825-06b61108e31a",
   "metadata": {},
   "source": [
    "# Update Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900c271d-5c99-4b01-81ca-ad69a5f2eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    \"\"\"\n",
    "        Explaining Epsilon-Greedy Logic: \n",
    "        Since epsilon starts at 1.0, and our random number generator doesn't include 1.0 and\n",
    "        we're using an exponential decay function for our epsilon, for the beginning we will \n",
    "        be mostly exploring a lot (AKA choosing the random action). However, as epsilon gets\n",
    "        smaller over time, we will be exploiting what we've learned more and more, and exploring\n",
    "        only very little.\n",
    "    \"\"\"\n",
    "\n",
    "    random_number = np.random.random()\n",
    "\n",
    "    # EXPLOITATION (choose the action that maximizes Q)\n",
    "    if random_number > epsilon:\n",
    "\n",
    "        # For the given (discrete) state, grab that row\n",
    "        state_row = q_table[discrete_state, :]\n",
    "\n",
    "        # Since the indeces correspond to the action 1:1, \n",
    "        # then argmax here works well for us since it returns\n",
    "        # the array index of the maximum value\n",
    "        action = np.argmax(state_row)\n",
    "        \n",
    "    # EXPLORATION (choose a random action)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6538c96-292c-45f3-8ff3-ff098e1592fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "    \"\"\"\n",
    "        Q-Learning Update Equation\n",
    "    \"\"\"\n",
    "    return old_q_value + ALPHA*(reward + GAMMA*next_optimal_q_value - old_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a3e5dca-909e-463f-b682-dbe6910ce0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon, episode):\n",
    "    \"\"\"\n",
    "        Exponential Decay for the epsilon parameter\n",
    "    \"\"\"\n",
    "    return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156063e5-6d3c-4f69-a4e4-b4056e0cabb2",
   "metadata": {},
   "source": [
    "# Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281e2f39-119b-4abe-aba6-8bdb525b279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of our rewards: 289.0\n",
      "Total sum of our rewards: 984.0\n",
      "Total sum of our rewards: 1879.0\n",
      "Total sum of our rewards: 2835.0\n",
      "Total sum of our rewards: 3806.0\n",
      "Total sum of our rewards: 4795.0\n",
      "Total sum of our rewards: 5784.0\n",
      "Total sum of our rewards: 6776.0\n",
      "Total sum of our rewards: 7767.0\n",
      "Total sum of our rewards: 8757.0\n",
      "Total sum of our rewards: 9740.0\n",
      "Total sum of our rewards: 10731.0\n",
      "Total sum of our rewards: 11722.0\n",
      "Total sum of our rewards: 12716.0\n",
      "Total sum of our rewards: 13705.0\n",
      "Total sum of our rewards: 14697.0\n",
      "Total sum of our rewards: 15688.0\n",
      "Total sum of our rewards: 16680.0\n",
      "Total sum of our rewards: 17673.0\n",
      "Total sum of our rewards: 18665.0\n"
     ]
    }
   ],
   "source": [
    "# Just to keep track of rewards\n",
    "rewards = []\n",
    "log_interval = 1000\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\n",
    "    # For every episode, reset environment and total rewards\n",
    "    state = env.reset()[0] # env.reset() returns [observation, info], we just want the observation\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    # Agent plays the game\n",
    "    while not done:\n",
    "        # 3 ways to be done:\n",
    "        # Winning the game\n",
    "        # Losing the game (falling in a hole)\n",
    "        # Hitting the max_episodes_steps in the environment delcaration\n",
    "\n",
    "        # Get ACTION\n",
    "        action = epsilon_greedy_action_selection(epsilon, q_table, state)\n",
    "\n",
    "        # Perform ACTION\n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # OLD (current) Q Value Q(st, at)\n",
    "        old_q_value = q_table[state, action]\n",
    "\n",
    "        # Get next optimal Q Value (what's the max Q value for this state) Q(st+1, at+1)\n",
    "        next_optimal_q_value = np.max(q_table[new_state, :])\n",
    "\n",
    "        # Compute the next Q Value\n",
    "        next_q_value = compute_next_q_value(old_q_value, reward, next_optimal_q_value) \n",
    "\n",
    "        # Update the Q Table\n",
    "        q_table[state, action] = next_q_value\n",
    "\n",
    "        # Track the rewards\n",
    "        total_rewards = total_rewards + reward\n",
    "\n",
    "        # new_state is not the state\n",
    "        state = new_state\n",
    "\n",
    "    # Agent finished a game\n",
    "    episode += 1\n",
    "\n",
    "    # We want to reduce epsilon after each game, not DURING the game\n",
    "    epsilon = reduce_epsilon(epsilon, episode)\n",
    "\n",
    "    # For plotting purposes, keep track of rewards\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "    # To make sure it's working\n",
    "    if episode % log_interval == 0:\n",
    "        print(f'Total sum of our rewards: {np.sum(rewards)}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4d297-28dd-4710-bb21-b4332f53e42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
